name: CI/CD Pipeline

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]

jobs:
  ci-cd:
    runs-on: ubuntu-latest
    steps:
      # 1️⃣ Checkout repo
      - name: Checkout code
        uses: actions/checkout@v3

      # 2️⃣ Setup Python
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: 3.11

      # 3️⃣ Install dependencies
      - name: Install dependencies
        run: |
          pip install -r requirements.txt
          pip install databricks-cli==0.18.0 jq

      # 4️⃣ (Optional) Run scripts locally with MLflow pointing to Databricks
      - name: Run local scripts (with Databricks tracking)
        env:
          MLFLOW_TRACKING_URI: databricks
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
        run: |
          python notebooks/01_data_prep.py
          python notebooks/02_train.py
          python notebooks/03_evaluate.py
          python notebooks/04_register_model.py

      # 5️⃣ Create or update Databricks Job
      - name: Deploy Databricks Job
        run: |
          databricks jobs reset --json-file jobs/titanic_job.yaml || databricks jobs create --json-file jobs/titanic_job.yaml
        env:
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}

      # 6️⃣ Trigger Databricks Job Run safely
      - name: Run Databricks Job
        run: |
          JOB_ID=$(databricks jobs list --output JSON | jq -r '.jobs[] | select(.settings.name=="titanic_pipeline") | .job_id')
          echo "Running Job ID: $JOB_ID"
          databricks jobs run-now --job-id $JOB_ID
        env:
          DATABRICKS_HOST: ${{ secrets.DATABRICKS_HOST }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
